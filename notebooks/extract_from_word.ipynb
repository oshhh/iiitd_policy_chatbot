{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doc_tree(filename):\n",
    "    pdf_file = filename + '.pdf'\n",
    "    docx_file = filename + '.docx'\n",
    "    # if docx doesn't exist\n",
    "    # parse(pdf_file, docx_file)\n",
    "\n",
    "    document = Document(docx_file)\n",
    "    id_ = 0\n",
    "    text = {}\n",
    "    adj_list = {}\n",
    "    parent = {}\n",
    "    prev_titles = []\n",
    "    for para in document.paragraphs:\n",
    "        if para.text:\n",
    "            id_ += 1\n",
    "            # get size\n",
    "            size = 0\n",
    "            if para.style.font.size != None:\n",
    "                size = para.style.font.size/12700\n",
    "            for run in para.runs:\n",
    "                if run.font.size:\n",
    "                    size = max(size, run.font.size/12700)\n",
    "            # get prev title list\n",
    "            while len(prev_titles) and prev_titles[-1]['size'] >= size:\n",
    "                prev_titles.pop()\n",
    "            if prev_titles:\n",
    "                adj_list[prev_titles[-1]['id']].append(id_)\n",
    "                parent[id_] = prev_titles[-1]['id']\n",
    "            else:\n",
    "                parent[id_] = None\n",
    "            prev_titles.append({'size': size, 'id': id_})\n",
    "            adj_list[id_] = []\n",
    "            text[id_] = preprocess_paragraph(para.text)\n",
    "    return text, adj_list, parent\n",
    "\n",
    "def create_tree(document):\n",
    "    id_ = 0\n",
    "    text = {}\n",
    "    adj_list = {}\n",
    "    parent = {}\n",
    "    prev_titles = []\n",
    "    for element in document['content']:\n",
    "        id_ += 1\n",
    "        # get size\n",
    "        size = element[1]\n",
    "        # get prev title list\n",
    "        while len(prev_titles) and prev_titles[-1]['size'] >= size:\n",
    "            prev_titles.pop()\n",
    "        if prev_titles:\n",
    "            adj_list[prev_titles[-1]['id']].append(id_)\n",
    "            parent[id_] = prev_titles[-1]['id']\n",
    "        else:\n",
    "            parent[id_] = None\n",
    "        prev_titles.append({'size': size, 'id': id_})\n",
    "        adj_list[id_] = []\n",
    "        text[id_] = preprocess_paragraph(element[0])\n",
    "    return text, adj_list, parent\n",
    "\n",
    "def create_kg(documents):\n",
    "    subject_edges = set([])\n",
    "    modifier_edges = set([])\n",
    "    subject_modifier_edges = set([])\n",
    "    relation_edges = []\n",
    "    other_edges = set([])\n",
    "\n",
    "    did = 0\n",
    "    tid = 10000\n",
    "    pid = 20000\n",
    "    sid = 30000\n",
    "    eid = 40000\n",
    "    xid = 50000\n",
    "    count = 0\n",
    "    eid_to_text = {}\n",
    "    text_to_eid = {}\n",
    "    sid_to_text = {}\n",
    "    text_to_sid = {}\n",
    "    pid_to_text = {}\n",
    "    text_to_pid = {}\n",
    "    tid_to_text = {}\n",
    "    text_to_tid = {}\n",
    "    did_to_text = {}\n",
    "    text_to_did = {}\n",
    "    xid_to_text = {}\n",
    "    text_to_xid = {}\n",
    "    # for each document get the paragraphs and topics\n",
    "    for doc in tqdm(documents):\n",
    "        if doc['format'] != 'html' or not 'body' in doc:\n",
    "            continue\n",
    "        if doc['name'] in text_to_did:\n",
    "            continue\n",
    "        text_to_did[doc['name']] = did\n",
    "        did_to_text[did] = doc\n",
    "        did += 1\n",
    "        # read document in python\n",
    "        text, adj_list, parent = create_tree(doc)\n",
    "        \n",
    "        # traverse doc tree to identify paragraphs and topics in the document\n",
    "        for node in adj_list:\n",
    "            if adj_list[node]:\n",
    "                if text[node] not in text_to_tid:\n",
    "                    text_to_tid[text[node]] = tid\n",
    "                    tid_to_text[tid] = text[node]\n",
    "                    tid += 1\n",
    "                if parent[node]:\n",
    "                    if text[parent[node]] not in text_to_tid:\n",
    "                        text_to_tid[text[parent[node]]] = tid\n",
    "                        tid_to_text[tid] = text[parent[node]]\n",
    "                        tid += 1\n",
    "                    other_edges.add((text_to_tid[text[node]], 'about_concept', text_to_tid[text[parent[node]]]))\n",
    "            else:\n",
    "                if text[node] not in text_to_pid:\n",
    "                    text_to_pid[text[node]] = pid\n",
    "                    pid_to_text[pid] = text[node]\n",
    "                    pid += 1\n",
    "                if parent[node]:\n",
    "                    if text[parent[node]] not in text_to_tid:\n",
    "                        text_to_tid[text[parent[node]]] = tid\n",
    "                        tid_to_text[tid] = text[parent[node]]\n",
    "                        tid += 1\n",
    "                    other_edges.add((text_to_pid[text[node]], 'about_concept', text_to_tid[text[parent[node]]]))\n",
    "                    other_edges.add((text_to_pid[text[node]], 'from_document', text_to_did[doc['name']]))\n",
    "    # convert topics to lower case\n",
    "    for tid in tqdm(tid_to_text):\n",
    "        tid_to_text[tid] = {\n",
    "            'text': tid_to_text[tid].lower(),\n",
    "            'keywords': [k[0] for k in find_keywords(tid_to_text[tid].lower())],\n",
    "            'tags': list(set([tag for k in find_keywords(tid_to_text[tid].lower()) for tag in k[1]]))\n",
    "        }\n",
    "    \n",
    "    # for each paragraph get sentences\n",
    "    for pid in tqdm(pid_to_text):\n",
    "        sentences = split_into_sentences(pid_to_text[pid])\n",
    "        for sentence in sentences:\n",
    "            sentence = {\n",
    "                'text': sentence, \n",
    "                'stemmed_tokens': get_stemmed_sentence_tokens(sentence)\n",
    "            }\n",
    "            if sentence['text'] not in text_to_sid:\n",
    "                text_to_sid[sentence['text']] = sid\n",
    "                sid_to_text[sid] = sentence\n",
    "                sid += 1\n",
    "            other_edges.add((pid, 'contains_sentence', text_to_sid[sentence['text']]))\n",
    "    \n",
    "    # for each sentence get extractions\n",
    "    for sid in tqdm(sid_to_text):\n",
    "        extractions = extract(sid_to_text[sid]['text'])\n",
    "        if extractions:\n",
    "            count += 1\n",
    "            for extraction in extractions:\n",
    "                eid_to_text[eid] = extraction\n",
    "                other_edges.add((sid, 'contains_extraction', eid))\n",
    "                eid += 1\n",
    "        else:\n",
    "            keywords = find_keywords(sid_to_text[sid]['text'])\n",
    "            for keyword in keywords:\n",
    "                if keyword[0] not in text_to_xid:\n",
    "                    text_to_xid[keyword[0]] = xid\n",
    "                    xid_to_text[xid] = keyword\n",
    "                    xid += 1\n",
    "                other_edges.add((sid, 'about_entity', text_to_xid[keyword[0]]))\n",
    "    \n",
    "    # canonicalise the obtained extractions\n",
    "    canonicalise(eid_to_text)\n",
    "    \n",
    "    # for each extraction create entities and relations\n",
    "    for eid in tqdm(eid_to_text):\n",
    "        ext = eid_to_text[eid]\n",
    "        \n",
    "        if ext['subject'][0] not in text_to_xid:\n",
    "            text_to_xid[ext['subject'][0]] = xid\n",
    "            xid_to_text[xid] = ext['subject']\n",
    "            xid += 1\n",
    "        \n",
    "        subject_edges.add((eid, 'subject', text_to_xid[ext['subject'][0]]))\n",
    "\n",
    "        if ext['object'][0] not in text_to_xid:\n",
    "            text_to_xid[ext['object'][0]] = xid\n",
    "            xid_to_text[xid] = ext['object']\n",
    "            xid += 1\n",
    "\n",
    "        relation_edges.append((eid, ext['relation'], text_to_xid[ext['object'][0]], list(ext['rel_synsets'])))\n",
    "        \n",
    "        for modifier in ext['modifiers']:\n",
    "            if modifier['m_obj'][0] not in text_to_xid:\n",
    "                text_to_xid[modifier['m_obj'][0]] = xid\n",
    "                xid_to_text[xid] = modifier['m_obj']\n",
    "                xid += 1\n",
    "\n",
    "            modifier_edges.add((eid, modifier['m_rel'], text_to_xid[modifier['m_obj'][0]]))\n",
    "            \n",
    "        for subject_modifier in ext['subject_modifiers']:\n",
    "            if subject_modifier['m_obj'][0] not in text_to_xid:\n",
    "                text_to_xid[subject_modifier['m_obj'][0]] = xid\n",
    "                xid_to_text[xid] = subject_modifier['m_obj']\n",
    "                xid += 1\n",
    "\n",
    "            subject_modifier_edges.add((eid, subject_modifier['m_rel'], text_to_xid[subject_modifier['m_obj'][0]]))\n",
    "    \n",
    "    print(did, tid, pid, sid, eid, xid, count, round(10000 * count/sid)/100)\n",
    "    \n",
    "    offset = {\n",
    "        'documents': 0,\n",
    "        'topics': len(did_to_text),\n",
    "        'paragraphs': len(did_to_text) + len(tid_to_text),\n",
    "        'sentences': len(did_to_text) + len(tid_to_text) + len(pid_to_text),\n",
    "        'extractions': len(did_to_text) + len(tid_to_text) + len(pid_to_text) + len(sid_to_text),\n",
    "        'entities':  len(did_to_text) + len(tid_to_text) + len(pid_to_text) + len(sid_to_text) + len(eid_to_text)\n",
    "    }\n",
    "    \n",
    "    vertices = {\n",
    "        'documents': list({'id': k, 'text': did_to_text[k]['name'], 'source': did_to_text[k]['link']} for k in did_to_text.keys()),\n",
    "        'topics': list({'id': k, 'text': tid_to_text[k]['text'], 'keywords': tid_to_text[k]['keywords'], 'tags': tid_to_text[k]['tags']} for k in tid_to_text.keys()),\n",
    "        'paragraphs': list({'id': k, 'text': pid_to_text[k]} for k in pid_to_text.keys()),\n",
    "        'sentences': list({'id': k, 'text': sid_to_text[k]['text'], 'stemmed_tokens': sid_to_text[k]['stemmed_tokens']} for k in sid_to_text.keys()),\n",
    "        'extractions': list({'id': k, 'body': eid_to_text[k]} for k in eid_to_text.keys()),\n",
    "        'entities': list({'id': k, 'text': xid_to_text[k][0], 'tags': xid_to_text[k][1], 'tokens': xid_to_text[k][2]} for k in xid_to_text.keys())\n",
    "    }\n",
    "    \n",
    "    triples = {\n",
    "        'main': list(other_edges),\n",
    "        'subjects': list(subject_edges),\n",
    "        'modifiers': list(modifier_edges),\n",
    "        'subject_modifiers': list(subject_modifier_edges),\n",
    "        'relations': list(relation_edges)\n",
    "    }\n",
    "    return vertices, triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[{'subject': 'I', 'relation': 'create', 'object': 'games', 'modifiers': [], 'subject_modifiers': [], 'condition': None}, {'subject': 'I', 'relation': 'create', 'object': 'puzzles', 'modifiers': [], 'subject_modifiers': [], 'condition': None}, {'subject': 'I', 'relation': 'play', 'object': 'games', 'modifiers': [], 'subject_modifiers': [], 'condition': None}, {'subject': 'I', 'relation': 'play', 'object': 'puzzles', 'modifiers': [], 'subject_modifiers': [], 'condition': None}, {'subject': 'I', 'relation': 'play', 'object': 'sports', 'modifiers': [], 'subject_modifiers': [], 'condition': None}]\n",
      "[{'subject': 'I', 'relation': 'create', 'object': 'games', 'modifiers': [], 'subject_modifiers': [], 'condition': 'I have time'}]\n",
      "[{'subject': 'I', 'relation': 'create', 'object': 'games', 'modifiers': [], 'subject_modifiers': [], 'condition': 'I have time'}]\n",
      "[{'subject': 'I', 'relation': 'have', 'object': 'time', 'modifiers': [{'m_rel': 'which is used for', 'm_obj': 'games'}], 'subject_modifiers': [], 'condition': None}]\n",
      "[{'subject': 'Any problem', 'relation': 'shall be referred to', 'object': 'the UG committee', 'modifiers': [{'m_rel': 'which may refer', 'm_obj': 'it'}, {'m_rel': 'to', 'm_obj': 'the Senate'}], 'subject_modifiers': [], 'condition': None}]\n",
      "There will not be any late registration in the summer term and a student shall not be allowed to add a course after registration.\n",
      "('AAANNAN,NVCNNNAN', ['After', 'graduating', 'from', 'Columbia', 'University', 'in', '1983', ',', 'he', 'worked', 'as', 'a', 'community', 'organizer', 'in', 'Chicago'])\n"
     ]
    }
   ],
   "source": [
    "print(extract('Any condition arising in the B.Tech. program and not covered in the regulations shall be referred to the UG committee.'))\n",
    "print(extract('I create and play games and puzzles and play sports.'))\n",
    "print(extract('I create games if I have time.'))\n",
    "print(extract('If I have time I create games.'))\n",
    "print(extract('I have time which is used for games.'))\n",
    "print(extract('Any problem shall be referred to the UG committee which may refer it to the Senate.'))\n",
    "print(nlp('There will not be any late registration in the summer term and a student shall not be allowed to add a course after registration.'))\n",
    "print(get_sentence_structure('After graduating from Columbia University in 1983, he worked as a community organizer in Chicago'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  2.75it/s]\n",
      "100%|██████████| 70/70 [00:07<00:00,  9.36it/s]\n",
      "100%|██████████| 164/164 [00:06<00:00, 27.00it/s]\n",
      "100%|██████████| 289/289 [00:22<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: verbatim\n",
      "ERROR: should\n",
      "ERROR: should\n",
      "ERROR: should\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:00<00:00, 143339.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 10069 20163 30288 40087 50772 62 0.2\n",
      "did, tid, pid, sid, eid, count, percentage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "triples = None\n",
    "vertices = None\n",
    "doc_names = [\"UG Regulations\", \"Academic Dishonesty Policy | IIIT-Delhi\", \"Evaluation Policy | IIIT-Delhi\", \"Placement Procedure & Policies | IIIT-Delhi\", \"Green Policy | IIIT-Delhi\", \"Privacy Policy | IIIT-Delhi\", \"Student Conduct Policy | IIIT-Delhi\", \"Refund / Cancellation Policy | IIIT-Delhi\", \"Hostel Policies | IIIT-Delhi\", \"Allocation Policies | IIIT-Delhi\", \"Internships @ IIIT-D | IIIT-Delhi\", \"Disciplinary Action | IIIT-Delhi\", \"Facility Management Services | IIIT-Delhi\", \"B.Tech. Fee Waiver | IIIT-Delhi\"]\n",
    "documents = read_json('../data/files/iiit_website_content.json')[:16]\n",
    "vertices, triples = create_kg(documents)\n",
    "print('did, tid, pid, sid, eid, count, percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json({'vertices': vertices, 'edges': triples}, '../neo4j/iiit_website_graph.json')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 101 361 725 207 588 done \n",
    "1 87 300 665 202 582 done remove paras with < 5 words\n",
    "1 87 284 648 202 582 done remove paras with < 6 words\n",
    "1 86 264 628 199 579 done remove paras with < 7 words\n",
    "2 91 307 697 241 671 done with 2 docs (+69 sentences, +42 extractions)\n",
    "3 92 318 721 256 723 done with 3 docs (+24 sentences, +15 extractions)\n",
    "3 92 318 721 256 717 done word sense disambiguation (-6 entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp('graduation').similarity(nlp('pass')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp('graduation')[0].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
