{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doc_tree(filename):\n",
    "    pdf_file = filename + '.pdf'\n",
    "    docx_file = filename + '.docx'\n",
    "    # if docx doesn't exist\n",
    "    # parse(pdf_file, docx_file)\n",
    "\n",
    "    document = Document(docx_file)\n",
    "    id_ = 0\n",
    "    text = {}\n",
    "    adj_list = {}\n",
    "    parent = {}\n",
    "    prev_titles = []\n",
    "    for para in document.paragraphs:\n",
    "        if para.text:\n",
    "            id_ += 1\n",
    "            # get size\n",
    "            size = 0\n",
    "            if para.style.font.size != None:\n",
    "                size = para.style.font.size/12700\n",
    "            for run in para.runs:\n",
    "                if run.font.size:\n",
    "                    size = max(size, run.font.size/12700)\n",
    "            # get prev title list\n",
    "            while len(prev_titles) and prev_titles[-1]['size'] <= size:\n",
    "                prev_titles.pop()\n",
    "            if prev_titles:\n",
    "                adj_list[prev_titles[-1]['id']].append(id_)\n",
    "                parent[id_] = prev_titles[-1]['id']\n",
    "            else:\n",
    "                parent[id_] = None\n",
    "            prev_titles.append({'size': size, 'id': id_})\n",
    "            adj_list[id_] = []\n",
    "            text[id_] = preprocess_paragraph(para.text)\n",
    "    return text, adj_list, parent\n",
    "\n",
    "def create_kg(documents):\n",
    "    subject_edges = set([])\n",
    "    modifier_edges = set([])\n",
    "    subject_modifier_edges = set([])\n",
    "    relation_edges = []\n",
    "    other_edges = set([])\n",
    "\n",
    "    did = 0\n",
    "    tid = 10000\n",
    "    pid = 20000\n",
    "    sid = 30000\n",
    "    eid = 40000\n",
    "    xid = 50000\n",
    "    count = 0\n",
    "    eid_to_text = {}\n",
    "    text_to_eid = {}\n",
    "    sid_to_text = {}\n",
    "    text_to_sid = {}\n",
    "    pid_to_text = {}\n",
    "    text_to_pid = {}\n",
    "    tid_to_text = {}\n",
    "    text_to_tid = {}\n",
    "    did_to_text = {}\n",
    "    text_to_did = {}\n",
    "    xid_to_text = {}\n",
    "    text_to_xid = {}\n",
    "    # for each document get the paragraphs and topics\n",
    "    for doc in documents:\n",
    "        if doc not in text_to_did:\n",
    "            text_to_did[doc] = did\n",
    "            did_to_text[did] = doc\n",
    "            did += 1\n",
    "        # read document in python\n",
    "        text, adj_list, parent = create_doc_tree(doc)\n",
    "        \n",
    "        # traverse doc tree to identify paragraphs and topics in the document\n",
    "        for node in adj_list:\n",
    "            if adj_list[node]:\n",
    "                if text[node] not in text_to_tid:\n",
    "                    text_to_tid[text[node]] = tid\n",
    "                    tid_to_text[tid] = text[node]\n",
    "                    tid += 1\n",
    "                if parent[node]:\n",
    "                    if text[parent[node]] not in text_to_tid:\n",
    "                        text_to_tid[text[parent[node]]] = tid\n",
    "                        tid_to_text[tid] = text[parent[node]]\n",
    "                        tid += 1\n",
    "                    other_edges.add((text_to_tid[text[node]], 'about_concept', text_to_tid[text[parent[node]]]))\n",
    "            else:\n",
    "                if text[node] not in text_to_pid:\n",
    "                    text_to_pid[text[node]] = pid\n",
    "                    pid_to_text[pid] = text[node]\n",
    "                    pid += 1\n",
    "                if text[parent[node]] not in text_to_tid:\n",
    "                    text_to_tid[text[parent[node]]] = tid\n",
    "                    tid_to_text[tid] = text[parent[node]]\n",
    "                    tid += 1\n",
    "                other_edges.add((text_to_pid[text[node]], 'about_concept', text_to_tid[text[parent[node]]]))\n",
    "                other_edges.add((text_to_pid[text[node]], 'from_document', text_to_did[doc]))\n",
    "    \n",
    "    # convert topics to lower case\n",
    "    for tid in tid_to_text:\n",
    "        tid_to_text[tid] = {\n",
    "            'text': tid_to_text[tid].lower(),\n",
    "            'keywords': [k[0] for k in find_keywords(tid_to_text[tid].lower())],\n",
    "            'tags': list(set([tag for k in find_keywords(tid_to_text[tid].lower()) for tag in k[1]]))\n",
    "        }\n",
    "    \n",
    "    # for each paragraph get sentences\n",
    "    for pid in pid_to_text:\n",
    "        sentences = split_into_sentences(pid_to_text[pid])\n",
    "        for sentence in sentences:\n",
    "            if sentence not in text_to_sid:\n",
    "                text_to_sid[sentence] = sid\n",
    "                sid_to_text[sid] = sentence\n",
    "                sid += 1\n",
    "            other_edges.add((pid, 'contains_sentence', text_to_sid[sentence]))\n",
    "    \n",
    "    # for each sentence get extractions\n",
    "    for sid in sid_to_text:\n",
    "        extractions = extract(sid_to_text[sid])\n",
    "        if extractions:\n",
    "            count += 1\n",
    "            for extraction in extractions:\n",
    "                eid_to_text[eid] = extraction\n",
    "                other_edges.add((sid, 'contains_extraction', eid))\n",
    "                eid += 1\n",
    "        else:\n",
    "            keywords = find_keywords(sid_to_text[sid])\n",
    "            for keyword in keywords:\n",
    "                if keyword[0] not in text_to_xid:\n",
    "                    text_to_xid[keyword[0]] = xid\n",
    "                    xid_to_text[xid] = keyword\n",
    "                    xid += 1\n",
    "                other_edges.add((sid, 'about_entity', text_to_xid[keyword[0]]))\n",
    "    \n",
    "    # canonicalise the obtained extractions\n",
    "    canonicalise(eid_to_text)\n",
    "    \n",
    "    # for each extraction create entities and relations\n",
    "    for eid in eid_to_text:\n",
    "        ext = eid_to_text[eid]\n",
    "        \n",
    "        if ext['subject'][0] not in text_to_xid:\n",
    "            text_to_xid[ext['subject'][0]] = xid\n",
    "            xid_to_text[xid] = ext['subject']\n",
    "            xid += 1\n",
    "        \n",
    "        subject_edges.add((eid, 'subject', text_to_xid[ext['subject'][0]]))\n",
    "\n",
    "        if ext['object'][0] not in text_to_xid:\n",
    "            text_to_xid[ext['object'][0]] = xid\n",
    "            xid_to_text[xid] = ext['object']\n",
    "            xid += 1\n",
    "\n",
    "        relation_edges.append((eid, ext['relation'], text_to_xid[ext['object'][0]], list(ext['rel_synsets'])))\n",
    "        \n",
    "        for modifier in ext['modifiers']:\n",
    "            if modifier['m_obj'][0] not in text_to_xid:\n",
    "                text_to_xid[modifier['m_obj'][0]] = xid\n",
    "                xid_to_text[xid] = modifier['m_obj']\n",
    "                xid += 1\n",
    "\n",
    "            modifier_edges.add((eid, modifier['m_rel'], text_to_xid[modifier['m_obj'][0]]))\n",
    "            \n",
    "        for subject_modifier in ext['subject_modifiers']:\n",
    "            if subject_modifier['m_obj'][0] not in text_to_xid:\n",
    "                text_to_xid[subject_modifier['m_obj'][0]] = xid\n",
    "                xid_to_text[xid] = subject_modifier['m_obj']\n",
    "                xid += 1\n",
    "\n",
    "            subject_modifier_edges.add((eid, subject_modifier['m_rel'], text_to_xid[subject_modifier['m_obj'][0]]))\n",
    "    \n",
    "    print(did, tid, pid, sid, eid, xid, count, round(10000 * count/sid)/100)\n",
    "    \n",
    "    offset = {\n",
    "        'documents': 0,\n",
    "        'topics': len(did_to_text),\n",
    "        'paragraphs': len(did_to_text) + len(tid_to_text),\n",
    "        'sentences': len(did_to_text) + len(tid_to_text) + len(pid_to_text),\n",
    "        'extractions': len(did_to_text) + len(tid_to_text) + len(pid_to_text) + len(sid_to_text),\n",
    "        'entities':  len(did_to_text) + len(tid_to_text) + len(pid_to_text) + len(sid_to_text) + len(eid_to_text)\n",
    "    }\n",
    "    \n",
    "    vertices = {\n",
    "        'documents': list({'id': k, 'text': did_to_text[k]} for k in did_to_text.keys()),\n",
    "        'topics': list({'id': k, 'text': tid_to_text[k]['text'], 'keywords': tid_to_text[k]['keywords'], 'tags': tid_to_text[k]['tags']} for k in tid_to_text.keys()),\n",
    "        'paragraphs': list({'id': k, 'text': pid_to_text[k]} for k in pid_to_text.keys()),\n",
    "        'sentences': list({'id': k, 'text': sid_to_text[k]} for k in sid_to_text.keys()),\n",
    "        'extractions': list({'id': k, 'body': eid_to_text[k]} for k in eid_to_text.keys()),\n",
    "        'entities': list({'id': k, 'text': xid_to_text[k][0], 'tags': xid_to_text[k][1], 'tokens': xid_to_text[k][2]} for k in xid_to_text.keys())\n",
    "    }\n",
    "    \n",
    "    triples = {\n",
    "        'main': list(other_edges),\n",
    "        'subjects': list(subject_edges),\n",
    "        'modifiers': list(modifier_edges),\n",
    "        'subject_modifiers': list(subject_modifier_edges),\n",
    "        'relations': list(relation_edges)\n",
    "    }\n",
    "    return vertices, triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[{'subject': 'I', 'relation': 'create', 'object': 'games', 'modifiers': [], 'subject_modifiers': [], 'condition': None}, {'subject': 'I', 'relation': 'create', 'object': 'puzzles', 'modifiers': [], 'subject_modifiers': [], 'condition': None}, {'subject': 'I', 'relation': 'play', 'object': 'games', 'modifiers': [], 'subject_modifiers': [], 'condition': None}, {'subject': 'I', 'relation': 'play', 'object': 'puzzles', 'modifiers': [], 'subject_modifiers': [], 'condition': None}, {'subject': 'I', 'relation': 'play', 'object': 'sports', 'modifiers': [], 'subject_modifiers': [], 'condition': None}]\n",
      "[{'subject': 'I', 'relation': 'create', 'object': 'games', 'modifiers': [], 'subject_modifiers': [], 'condition': 'I have time'}]\n",
      "[{'subject': 'I', 'relation': 'create', 'object': 'games', 'modifiers': [], 'subject_modifiers': [], 'condition': 'I have time'}]\n",
      "[{'subject': 'I', 'relation': 'have', 'object': 'time', 'modifiers': [{'m_rel': 'which is used for', 'm_obj': 'games'}], 'subject_modifiers': [], 'condition': None}]\n",
      "[{'subject': 'Any problem', 'relation': 'shall be referred to', 'object': 'the UG committee', 'modifiers': [{'m_rel': 'which may refer', 'm_obj': 'it'}, {'m_rel': 'to', 'm_obj': 'the Senate'}], 'subject_modifiers': [], 'condition': None}]\n",
      "There will not be any late registration in the summer term and a student shall not be allowed to add a course after registration.\n",
      "('AAANNAN,NVCNNNAN', ['After', 'graduating', 'from', 'Columbia', 'University', 'in', '1983', ',', 'he', 'worked', 'as', 'a', 'community', 'organizer', 'in', 'Chicago'])\n"
     ]
    }
   ],
   "source": [
    "print(extract('Any condition arising in the B.Tech. program and not covered in the regulations shall be referred to the UG committee.'))\n",
    "print(extract('I create and play games and puzzles and play sports.'))\n",
    "print(extract('I create games if I have time.'))\n",
    "print(extract('If I have time I create games.'))\n",
    "print(extract('I have time which is used for games.'))\n",
    "print(extract('Any problem shall be referred to the UG committee which may refer it to the Senate.'))\n",
    "print(nlp('There will not be any late registration in the summer term and a student shall not be allowed to add a course after registration.'))\n",
    "print(get_sentence_structure('After graduating from Columbia University in 1983, he worked as a community organizer in Chicago'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: should\n",
      "ERROR: expulsion\n",
      "1 10059 20132 30290 40171 50760 118 0.39\n",
      "did, tid, pid, sid, eid, count, percentage\n"
     ]
    }
   ],
   "source": [
    "triples = None\n",
    "vertices = None\n",
    "documents = ['../data/files/UG-Regulations']\n",
    "\n",
    "vertices, triples = create_kg(documents)\n",
    "print('did, tid, pid, sid, eid, count, percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json({'vertices': vertices, 'edges': triples}, '../neo4j/graph.json')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 101 361 725 207 588 done \n",
    "1 87 300 665 202 582 done remove paras with < 5 words\n",
    "1 87 284 648 202 582 done remove paras with < 6 words\n",
    "1 86 264 628 199 579 done remove paras with < 7 words\n",
    "2 91 307 697 241 671 done with 2 docs (+69 sentences, +42 extractions)\n",
    "3 92 318 721 256 723 done with 3 docs (+24 sentences, +15 extractions)\n",
    "3 92 318 721 256 717 done word sense disambiguation (-6 entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.657118382755142\n"
     ]
    }
   ],
   "source": [
    "print(nlp('procedure').similarity(nlp('process')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
