{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doc_tree(filename):\n",
    "    pdf_file = filename + '.pdf'\n",
    "    docx_file = filename + '.docx'\n",
    "    # if docx doesn't exist\n",
    "    # parse(pdf_file, docx_file)\n",
    "\n",
    "    document = Document(docx_file)\n",
    "    id_ = 0\n",
    "    text = {}\n",
    "    adj_list = {}\n",
    "    parent = {}\n",
    "    prev_titles = []\n",
    "    for para in document.paragraphs:\n",
    "        if para.text:\n",
    "            id_ += 1\n",
    "            # get size\n",
    "            size = 0\n",
    "            if para.style.font.size != None:\n",
    "                size = para.style.font.size/12700\n",
    "            for run in para.runs:\n",
    "                if run.font.size:\n",
    "                    size = max(size, run.font.size/12700)\n",
    "            # get prev title list\n",
    "            while len(prev_titles) and prev_titles[-1]['size'] <= size:\n",
    "                prev_titles.pop()\n",
    "            if prev_titles:\n",
    "                adj_list[prev_titles[-1]['id']].append(id_)\n",
    "                parent[id_] = prev_titles[-1]['id']\n",
    "            else:\n",
    "                parent[id_] = None\n",
    "            prev_titles.append({'size': size, 'id': id_})\n",
    "            adj_list[id_] = []\n",
    "            text[id_] = preprocess_paragraph(para.text)\n",
    "    return text, adj_list, parent\n",
    "\n",
    "def create_kg(documents):\n",
    "    subject_edges = set([])\n",
    "    modifier_edges = set([])\n",
    "    subject_modifier_edges = set([])\n",
    "    relation_edges = []\n",
    "    other_edges = set([])\n",
    "\n",
    "    did = 0\n",
    "    sid = 0\n",
    "    eid = 0\n",
    "    pid = 0\n",
    "    tid = 0\n",
    "    count = 0\n",
    "    eid_to_text = {}\n",
    "    text_to_eid = {}\n",
    "    sid_to_text = {}\n",
    "    text_to_sid = {}\n",
    "    pid_to_text = {}\n",
    "    text_to_pid = {}\n",
    "    tid_to_text = {}\n",
    "    text_to_tid = {}\n",
    "    did_to_text = {}\n",
    "    text_to_did = {}\n",
    "    entities = set([])\n",
    "    # for each document get the paragraphs and topics\n",
    "    for doc in documents:\n",
    "        if doc not in text_to_did:\n",
    "            text_to_did[doc] = did\n",
    "            did_to_text[did] = doc\n",
    "            did += 1\n",
    "        # read document in python\n",
    "        text, adj_list, parent = create_doc_tree(doc)\n",
    "        \n",
    "        # traverse doc tree to identify paragraphs and topics in the document\n",
    "        for node in adj_list:\n",
    "            if adj_list[node]:\n",
    "                if text[node] not in text_to_tid:\n",
    "                    text_to_tid[text[node]] = tid\n",
    "                    tid_to_text[tid] = text[node]\n",
    "                    tid += 1\n",
    "                if parent[node]:\n",
    "                    if text[parent[node]] not in text_to_tid:\n",
    "                        text_to_tid[text[parent[node]]] = tid\n",
    "                        tid_to_text[tid] = text[parent[node]]\n",
    "                        tid += 1\n",
    "                    other_edges.add(('t_' + str(text_to_tid[text[node]]), 'about_concept', 't_' + str(text_to_tid[text[parent[node]]])))\n",
    "            else:\n",
    "                if text[node] not in text_to_pid:\n",
    "                    text_to_pid[text[node]] = pid\n",
    "                    pid_to_text[pid] = text[node]\n",
    "                    pid += 1\n",
    "                if text[parent[node]] not in text_to_tid:\n",
    "                    text_to_tid[text[parent[node]]] = tid\n",
    "                    tid_to_text[tid] = text[parent[node]]\n",
    "                    tid += 1\n",
    "                other_edges.add(('p_' + str(text_to_pid[text[node]]), 'about_concept', 't_' + str(text_to_tid[text[parent[node]]])))\n",
    "                other_edges.add(('p_' + str(text_to_pid[text[node]]), 'from_document', 'd_' + str(text_to_did[doc])))\n",
    "    \n",
    "    # for each paragraph get sentences\n",
    "    for pid in pid_to_text:\n",
    "        sentences = split_into_sentences(pid_to_text[pid])\n",
    "        for sentence in sentences:\n",
    "            if sentence not in text_to_sid:\n",
    "                text_to_sid[sentence] = sid\n",
    "                sid_to_text[sid] = sentence\n",
    "                sid += 1\n",
    "            other_edges.add(('p_' + str(pid), 'contains_sentence', 's_' + str(text_to_sid[sentence])))\n",
    "    \n",
    "    # for each sentence get extractions\n",
    "    for sid in sid_to_text:\n",
    "        extractions = extract(sid_to_text[sid])\n",
    "        if extractions:\n",
    "            count += 1\n",
    "            for extraction in extractions:\n",
    "                eid_to_text[eid] = extraction\n",
    "                other_edges.add(('s_' + str(sid), 'contains_extraction', 'e_' + str(eid)))\n",
    "                eid += 1\n",
    "        else:\n",
    "            keywords = find_keywords(sid_to_text[sid])\n",
    "            for keyword in keywords:\n",
    "                other_edges.add(('s_' + str(sid), 'about_entity', keyword[0]))\n",
    "                entities.add(keyword[0])\n",
    "    \n",
    "    # canonicalise the obtained extractions\n",
    "    canonicalise(eid_to_text)\n",
    "    \n",
    "    # for each extraction create entities and relations\n",
    "    for eid in eid_to_text:\n",
    "        ext = eid_to_text[eid]\n",
    "        \n",
    "        subject_edges.add(('e_' + str(eid), 'subject', ext['subject']))\n",
    "        entities.add(ext['subject'])\n",
    "        \n",
    "        relation_edges.append(('e_' + str(eid), ext['relation'], ext['object'], list(ext['rel_synsets'])))\n",
    "        entities.add(ext['object'])\n",
    "        \n",
    "        for modifier in ext['modifiers']:\n",
    "            modifier_edges.add(('e_' + str(eid), modifier['m_rel'], modifier['m_obj']))\n",
    "            entities.add(modifier['m_obj'])\n",
    "            \n",
    "        for subject_modifier in ext['subject_modifiers']:\n",
    "            subject_modifier_edges.add(('e_' + str(eid), subject_modifier['m_rel'], subject_modifier['m_obj']))\n",
    "            entities.add(subject_modifier['m_obj'])\n",
    "    \n",
    "    print(did, tid, pid, sid, eid, count, round(10000 * count/sid)/100)\n",
    "    \n",
    "    vertices = {\n",
    "        'documents': list({'id': 'd_' + str(k), 'text': did_to_text[k]} for k in did_to_text.keys()),\n",
    "        'topics': list({'id': 't_' + str(k), 'text': tid_to_text[k]} for k in tid_to_text.keys()),\n",
    "        'paragraphs': list({'id': 'p_' + str(k), 'text': pid_to_text[k]} for k in pid_to_text.keys()),\n",
    "        'sentences': list({'id': 's_' + str(k), 'text': sid_to_text[k]} for k in sid_to_text.keys()),\n",
    "        'extractions': list({'id': 'e_' + str(k), 'body': eid_to_text[k]} for k in eid_to_text.keys()),\n",
    "        'entities': list(entities)\n",
    "    }\n",
    "    \n",
    "    triples = {\n",
    "        'main': list(other_edges),\n",
    "        'subjects': list(subject_edges),\n",
    "        'modifiers': list(modifier_edges),\n",
    "        'subject_modifiers': list(subject_modifier_edges),\n",
    "        'relations': list(relation_edges)\n",
    "    }\n",
    "    return vertices, triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[{'subject': 'I', 'relation': 'create', 'object': 'games', 'modifiers': [], 'subject_modifiers': [], 'condition': None}, {'subject': 'I', 'relation': 'create', 'object': 'puzzles', 'modifiers': [], 'subject_modifiers': [], 'condition': None}, {'subject': 'I', 'relation': 'play', 'object': 'games', 'modifiers': [], 'subject_modifiers': [], 'condition': None}, {'subject': 'I', 'relation': 'play', 'object': 'puzzles', 'modifiers': [], 'subject_modifiers': [], 'condition': None}, {'subject': 'I', 'relation': 'play', 'object': 'sports', 'modifiers': [], 'subject_modifiers': [], 'condition': None}]\n",
      "[{'subject': 'I', 'relation': 'create', 'object': 'games', 'modifiers': [], 'subject_modifiers': [], 'condition': 'I have time'}]\n",
      "[{'subject': 'I', 'relation': 'create', 'object': 'games', 'modifiers': [], 'subject_modifiers': [], 'condition': 'I have time'}]\n",
      "[{'subject': 'I', 'relation': 'have', 'object': 'time', 'modifiers': [{'m_rel': 'which is used for', 'm_obj': 'games'}], 'subject_modifiers': [], 'condition': None}]\n",
      "[{'subject': 'Any problem', 'relation': 'shall be referred to', 'object': 'the UG committee', 'modifiers': [{'m_rel': 'which may refer', 'm_obj': 'it'}, {'m_rel': 'to', 'm_obj': 'the Senate'}], 'subject_modifiers': [], 'condition': None}]\n",
      "There will not be any late registration in the summer term and a student shall not be allowed to add a course after registration.\n",
      "('AAANNAN,NVCNNNAN', ['After', 'graduating', 'from', 'Columbia', 'University', 'in', '1983', ',', 'he', 'worked', 'as', 'a', 'community', 'organizer', 'in', 'Chicago'])\n"
     ]
    }
   ],
   "source": [
    "print(extract('Any condition arising in the B.Tech. program and not covered in the regulations shall be referred to the UG committee.'))\n",
    "print(extract('I create and play games and puzzles and play sports.'))\n",
    "print(extract('I create games if I have time.'))\n",
    "print(extract('If I have time I create games.'))\n",
    "print(extract('I have time which is used for games.'))\n",
    "print(extract('Any problem shall be referred to the UG committee which may refer it to the Senate.'))\n",
    "print(nlp('There will not be any late registration in the summer term and a student shall not be allowed to add a course after registration.'))\n",
    "print(get_sentence_structure('After graduating from Columbia University in 1983, he worked as a community organizer in Chicago'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: should\n",
      "ERROR: expulsion\n",
      "1 60 132 290 171 118 40.69\n",
      "did, tid, pid, sid, eid, count, percentage\n"
     ]
    }
   ],
   "source": [
    "triples = None\n",
    "vertices = None\n",
    "documents = ['../data/files/UG-Regulations']\n",
    "\n",
    "vertices, triples = create_kg(documents)\n",
    "print('did, tid, pid, sid, eid, count, percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json({'vertices': vertices, 'edges': triples}, '../data/handbook_graph.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-9cd3be5a8dc6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-9cd3be5a8dc6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    0 101 361 725 207 588 done\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1 101 361 725 207 588 done \n",
    "1 87 300 665 202 582 done remove paras with < 5 words\n",
    "1 87 284 648 202 582 done remove paras with < 6 words\n",
    "1 86 264 628 199 579 done remove paras with < 7 words\n",
    "2 91 307 697 241 671 done with 2 docs (+69 sentences, +42 extractions)\n",
    "3 92 318 721 256 723 done with 3 docs (+24 sentences, +15 extractions)\n",
    "3 92 318 721 256 717 done word sense disambiguation (-6 entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when the course is replaced or repeated, the new grade will be used for computation of the CGPA\n"
     ]
    }
   ],
   "source": [
    "print(nlp('when the course is replaced or repeated, the new grade will be used for computation of the CGPA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
